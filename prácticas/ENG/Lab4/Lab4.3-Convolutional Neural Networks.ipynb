{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4.3: Convolutional Neural Networks (CNN)**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "The neural networks we have considered so far have always worked with *structured data*, meaning data that can be stored in tables. Nowadays, it is increasingly common to encounter problems that require handling **unstructured data**, such as images, text, or audio, with images being the most frequent type. In these kinds of problems, unlike those we've seen so far, the model inputs are not vectors of values extracted from a dataset, but rather images.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> Grayscale images are encoded as two-dimensional <b>matrices</b>, while color images are represented as three-dimensional <b>tensors</b>.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:1200px\">\n",
    "        <img src=\"https://i.imgur.com/urLXh6F.png\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "The models we have seen so far are designed to work with vectors, so they cannot work directly with images. If we wanted to use a classic model (such as logistic regression or SVM) with images, it would be necessary to transform the image into a vector. A common way to do this is to *flatten* the image. Flattening an image means converting the 2D matrix (or 3D tensor) into a one-dimensional vector by concatenating the rows (or color channels) of the image one after another.\n",
    "\n",
    "For example, a $28\\times28$ pixel image would be transformed into a vector of $784$ elements. This resulting vector could then be used as input for a classic model. Although this method may imply the loss of relevant spatial information from the original image, it represents a simple way to adapt classic models to image processing. In traditional networks with dense (fully connected) layers, such as the ones we have seen so far, the same process would be necessary. This means that, even for small images like the one in the example, we obtain large vectors, which requires learning a large number of weights or parameters.\n",
    "\n",
    "To solve these problems, **convolutional neural networks (CNNs)** have emerged, whose architecture is composed of two parts:\n",
    "\n",
    "* **Feature extractor**: This part is responsible for extracting the relevant features from the image, that is, it learns a low-dimensional vector that represents the image. It is composed of a series of **convolutional** layers and **pooling** layers. \n",
    "* **Fully connected part**: This part is responsible for solving, based on the learned vector, the problem to be solved (classification, regression, ...). It is composed of a series of dense (fully connected) layers, like the networks used previously.\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:1200px\">\n",
    "        <img src=\"https://i.imgur.com/vsSJBcu.png\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> The <i>convolutional part</i> or the <i>feature extractor</i> of a CNN is responsible for learning a <b>vector representation</b> of the image which will then be the input to the fully connected part responsible for making the final prediction.\n",
    "</div>\n",
    "\n",
    "### **Objective**\n",
    "In this lab, you will learn how to solve an image classification problem by creating a convolutional network with `tensorFlow` and `keras`.\n",
    "\n",
    "## **2. Set up GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accelerate training as much as possible, in this lab we will make use of the *GPUs* installed on the lab computers.\n",
    "\n",
    "So far, all the models we have trained have run on *CPU* because `tensorflow` runs on this device by default. This library only allows the use of *GPU* on Windows through **Windows Subsystem for Linux (WSL)**.\n",
    "\n",
    "This feature allows us to have a Linux environment directly on Windows without the need to create a virtual machine or set up dual boot. As you will see, we can run Linux command-line programs directly on Windows.\n",
    "\n",
    "##### **Activate and install WSL**\n",
    "\n",
    "You will need to open a `cmd.exe` command window and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsl --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indicate your **UO** as both the username and password.\n",
    "\n",
    "Inside the Linux console (Ubuntu by default), we will need to install `conda` and create the environment for the course.\n",
    "\n",
    "##### **Install conda**\n",
    "\n",
    "Download `miniconda` (a minimal version of `conda`) and install it.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>You will need to answer yes (Yes) to the question <i>Do you wish to update your shell profile to automatically initialize conda?</i>.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Close the WSL terminal and reopen it to update the environment variables.\n",
    "\n",
    "You can find it again in the Windows menu by searching for `WSL`.\n",
    "\n",
    "Next, we will recreate the `SSII` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name \"SSII\" python=3.10\n",
    "conda activate \"SSII\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary libraries within the environment, including the version of `tensorflow` with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipykernel pandas seaborn scikit-learn\n",
    "pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Visual Studio Code**\n",
    "\n",
    "At this point, we have two operating systems: native Windows and a Linux running within it. Our goal is to run this Notebook inside Linux.\n",
    "\n",
    "To do this, we need to tell Visual Studio Code **to connect to a different machine**. The easiest way to do this is:\n",
    "\n",
    "* Open a new VSCode window (`File > New Window`).\n",
    "* In the new VSCode window, at the bottom left, there is an icon of two opposing arrows.\n",
    "* Clicking on it will bring up a top menu, from which we select <i>Connect to WSL</i>.\n",
    "* If everything works, at the bottom left, something like **WSL: Ubuntu** will appear.\n",
    "\n",
    "Next, we will install the **VSCode extensions**. You had already installed them on Windows, but now you need to do it in the Linux distribution.\n",
    "\n",
    "* Install *Python* and *Jupyter*.\n",
    "\n",
    "##### **File System**\n",
    "\n",
    "Remember, we are now inside another machine, so we will need to store the practice notebooks within it.\n",
    "\n",
    "Windows makes this access much easier. If you open a file explorer, you simply need to go to the *Linux* folder that appears in the quick access on the left:\n",
    "\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:600px\">\n",
    "        <img src=\"https://i.imgur.com/dEqSKsg.png\" style=\"height:400px\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "* Create the `SSII` folder in `Linux/Ubuntu/home/**your UO**/`.\n",
    "* Copy this notebook into it.\n",
    "* Open that folder from VSCode (`File > Open Folder...`).\n",
    "* Open the notebook and select the `conda` environment you just created as the kernel.\n",
    "\n",
    "The easiest way to verify that `tensorflow` has access to the GPU is by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export TF_CPP_MIN_LOG_LEVEL = 2  # To make tensorflow show only warnings and errors\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 2533\n",
    "\n",
    "print('-'*50)\n",
    "print('Num GPUs available: ', len(tf.config.list_physical_devices('GPU')))\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the current GPU usage with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to monitor the GPU usage continuously, you can open a WSL terminal (from Windows or from VSCode in `Terminal > New Terminal`) and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! watch -n1 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command simply executes the previous command every 1 second. To exit, you will need to press `Ctrl+C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **3. Convolutional Neural Networks**\n",
    "\n",
    "Once everything is set up, let's propose a potential problem.\n",
    "\n",
    "Imagine you work for the IT department at the post office, and currently, they have a person responsible for manually reading the recipients of letters (written by hand) and placing them, based on the postal code, in one box or another.\n",
    "\n",
    "As you can see, this process is very slow, so they propose installing a camera that takes images of each letter and automatically reads the postal codes.\n",
    "\n",
    "They already have images of the postal codes of the letters, but they don't know how to extract the digits from each image, so they present you with the following problem:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Create a model that, given an image of a handwritten number, can recognize which number it is.</b>\n",
    "</div>\n",
    "\n",
    "### **3.1 Data Preprocessing**\n",
    "\n",
    "To help you, they have already created a manually curated dataset that contains images of individual digits and labels indicating the number that appears in the image.\n",
    "\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:1000px\">\n",
    "        <img src=\"https://i.imgur.com/cHlLRXB.png\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "You can download the dataset using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains $70,000$ grayscale images of $28\\times28$ handwritten digits from $0$ to $10$.\n",
    "\n",
    "As you can see, it is already divided into $X$, $Y$, as well as into training ($60,000$) and test ($10,000$) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify, let's keep only 20,000 random test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed)\n",
    "indices = np.random.choice(len(x_train), size=20000, replace=False)\n",
    "\n",
    "# Obtain the samples\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Manually normalize the data between 0 and 1.\n",
    "    <hr>\n",
    "    The classes from <code>scikit-learn</code> do not work in this case, as they are designed to work with vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot to view one of the examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(x_train[0], cmap = 'Greys')\n",
    "plt.title(f'Example image of digit {y_train[1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Visualize image 1523 from the test set. Add its real label (class) in the title.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Machine Learning**\n",
    "\n",
    "Remember, to tackle this problem with classic machine learning models, we first need to transform the $28\\times28$ images into vectors.\n",
    "\n",
    "We create two new variables to store the original images and their \"flattened\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector = x_train.reshape(-1, 28 * 28)\n",
    "x_test_vector = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been adapted, we can attempt to tackle the problem with classic machine learning models.\n",
    "\n",
    "To determine which metric is most appropriate, let's first analyze how many images there are of each number (class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = y_train, hue = map(str, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the classes are fairly balanced, so the `f1 macro` metric will be sufficient.\n",
    "\n",
    "Keep in mind that methods other than baselines will take some time to train, as now each example consists of $784$ inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tabulate import tabulate\n",
    "    \n",
    "def evaluate_model(Y_test, preds_test, model_name, average = 'binary'):\n",
    "    preds_test = (preds_test >= 0.5).astype(int)\n",
    "    metrics = [\n",
    "        ('Accuracy', accuracy_score(Y_test, preds_test)),\n",
    "        ('F1', f1_score(Y_test,preds_test, average = average))\n",
    "    ]\n",
    "    \n",
    "    print(f'Results for {model_name}:')\n",
    "    print(tabulate(metrics, headers = ['Metric', 'TEST'], tablefmt = 'rounded_outline'))\n",
    "    print()\n",
    "    \n",
    "# Baseline Random\n",
    "baseline_random = DummyClassifier(strategy = 'uniform')\n",
    "baseline_random.fit(x_train_vector, y_train)\n",
    "preds_test = baseline_random.predict(x_test_vector)\n",
    "evaluate_model(y_test, preds_test, 'Baseline Random', average = 'macro')\n",
    "\n",
    "# Baseline Zero-R\n",
    "baseline_zero = DummyClassifier(strategy = 'most_frequent')\n",
    "baseline_zero.fit(x_train_vector, y_train)\n",
    "preds_test = baseline_zero.predict(x_test_vector)\n",
    "evaluate_model(y_test, preds_test, 'Baseline Zero-R', average = 'macro')\n",
    "\n",
    "# KNN\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_knn.fit(x_train_vector, y_train)\n",
    "preds_test = model_knn.predict(x_test_vector)\n",
    "evaluate_model(y_test, preds_test, 'KNN', average = 'macro')\n",
    "\n",
    "# Decision Trees\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_tree.fit(x_train_vector, y_train)\n",
    "preds_test = model_tree.predict(x_test_vector)\n",
    "evaluate_model(y_test, preds_test, 'Decision Tree', average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the following results:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model             | Accuracy (Test) | F1 (Test) |\n",
    "|-------------------|-----------------|-----------|\n",
    "| Baseline Random   | 0.114           | 0.032     |\n",
    "| Baseline Zero-R   | 0.114           | 0.020     |\n",
    "| KNN               | 0.211           | 0.120     |\n",
    "| Decision Tree     | 0.202           | 0.112     |\n",
    "\n",
    "</center>\n",
    "\n",
    "### **3.3 Deep Learning**\n",
    "\n",
    "We are going to create two options, a *fully connected network* that receives the $784$-dimensional vector, and then a *convolutional network* that directly takes the images.\n",
    "\n",
    "Before continuing, we will need to correctly encode the expected outputs $Y$ for our model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Encode and overwrite the Y using one-hot encoding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Fully Connected Network**\n",
    "\n",
    "We will first set the seeds for `tensorflow` and recreate the helper function `plot_loss_history()` to visualize the evolution of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "\n",
    "# Set the seeds for the libraries to ensure reproducibility of the results.\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    # Extract loss data from history\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss', None)  # May not exist if validation was not used\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # Create a DataFrame for seaborn\n",
    "    data = pd.DataFrame({ 'Epoch': list(epochs) * 2, 'Loss': loss + (val_loss if val_loss else []), 'Type': ['Train'] * len(loss) + (['Validation'] * len(val_loss) if val_loss else []) })\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.lineplot(data = data, x = 'Epoch', y = 'Loss', hue = 'Type')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Evolution During Training')\n",
    "    plt.legend(title = 'Dataset')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create a <i>fully connected network</i> to solve this problem and fill in the table. Adjust the hyperparameters if you wish.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model             | Accuracy (Test) | F1 (Test) |\n",
    "|-------------------|-----------------|-----------|\n",
    "| Baseline Random   | 0.114           | 0.032     |\n",
    "| Baseline Zero-R   | 0.114           | 0.020     |\n",
    "| KNN               | 0.211           | 0.120     |\n",
    "| Decision Tree     | 0.202           | 0.112     |\n",
    "| Neural Network    |                 |           |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def red_fully_connected(learning_rate):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "    optim = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model_fcn = red_fully_connected(learning_rate = 0.0005)\n",
    "\n",
    "# Show model summary\n",
    "model_fcn.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model_fcn.fit(x_train_vector, y_train, validation_split = 0.2, batch_size = 256, epochs = 20, verbose = 2)\n",
    "\n",
    "# Visualize loss history\n",
    "plot_loss_history(history)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "preds_test = model_fcn.predict(x_test_vector)\n",
    "evaluate_model(y_test, preds_test, 'Neural Network', average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Convolutional Neural Network**\n",
    "\n",
    "Below is the architecture of a convolutional neural network designed to solve this problem.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Complete the <i>input size</i>, the <i>activation function of the last layer</i>, and the <i>loss function</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def convolutional_network(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = ()))  # Input as grayscale image\n",
    "\n",
    "    # FEATURE EXTRACTOR -------------------------------------------\n",
    "    # First convolution and max pooling block\n",
    "    model.add(Conv2D(16, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    # Second convolution and max pooling block\n",
    "    model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    # Third convolution and max pooling block\n",
    "    model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    # FULLY CONNECTED PART ---------------------------------------------\n",
    "    # Flatten and final dense layers\n",
    "    model.add(Flatten())  # This vector is learned by the network during training and helps represent each image\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = '', name = 'output_layer'))\n",
    "\n",
    "    optim = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = '', optimizer = optim)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the network from scratch\n",
    "model_cnn = convolutional_network(learning_rate = 0.0005)\n",
    "\n",
    "# View the summary\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model_cnn.fit(x_train, y_train, validation_split = 0.2, batch_size = 128, epochs = 20, verbose = 2)\n",
    "\n",
    "# Visualize loss history\n",
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Add the necessary code to evaluate your model and fill in the table.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model             | Accuracy (Test) | F1 (Test) |\n",
    "|-------------------|-----------------|-----------|\n",
    "| Baseline Random   | 0.114           | 0.032     |\n",
    "| Baseline Zero-R   | 0.114           | 0.020     |\n",
    "| KNN               | 0.211           | 0.120     |\n",
    "| Decision Tree     | 0.202           | 0.112     |\n",
    "| Neural Network    |                 |           |\n",
    "| Convolutional Net |                 |           |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **4. Exercises**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Create a model that, given an image of a digit, predicts if <u>it is a four or a nine.</u> The code to create the necessary dataset is already provided.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def filter_and_relabel(x, y, class_pos = 1, class_neg = 7):\n",
    "    # Filter only the desired digits\n",
    "    idx = np.where((y == class_pos) | (y == class_neg))[0]\n",
    "    x_filtered = x[idx]\n",
    "    y_filtered = y[idx]\n",
    "\n",
    "    # Label: 1 if it's class_pos, 0 if it's class_neg\n",
    "    y_binary = (y_filtered == class_pos).astype(np.uint8)\n",
    "\n",
    "    # Balance (it should already be almost balanced)\n",
    "    idx_pos = np.where(y_binary == 1)[0]\n",
    "    idx_neg = np.where(y_binary == 0)[0]\n",
    "\n",
    "    n = min(len(idx_pos), len(idx_neg))\n",
    "    np.random.seed(42)\n",
    "    idx_pos_sampled = np.random.choice(idx_pos, size = n, replace = False)\n",
    "    idx_neg_sampled = np.random.choice(idx_neg, size = n, replace = False)\n",
    "\n",
    "    idx_total = np.concatenate([idx_pos_sampled, idx_neg_sampled])\n",
    "    np.random.shuffle(idx_total)\n",
    "\n",
    "    return x_filtered[idx_total], y_binary[idx_total]\n",
    "\n",
    "# Apply for train and test\n",
    "x_train, y_train = filter_and_relabel(x_train, y_train, class_pos = 4, class_neg = 9)\n",
    "x_test, y_test = filter_and_relabel(x_test, y_test, class_pos = 4, class_neg = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
